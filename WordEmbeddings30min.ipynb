{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What are Word Embeddings?\n",
    "\n",
    "*Word embeddings (WE's) are an excellent example of representational learning (RL) applied to Natural Language Processing (NLP).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\git_projects\\\\WordEmbeddings'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Representational Learning (RL)* is Unsupervised Learning - Why do it?\n",
    "\n",
    "<img src=\"./graphics/0.1 Good Representations make Learning easier.png\">\n",
    "\n",
    "### Figure 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 - Write function to read in GloVe embeddings\n",
    "\n",
    "<ul>\n",
    "  <li>Call this function <code>get_glove</code></li>\n",
    "  <li>Takes 3 parameters: <code>glove_dir</code>, <code>emb_dim</code> and <code>enc</code></li>\n",
    "    <ul>\n",
    "      <li><b>glove_dir</b> (str): directory containing the GloVe embeddings</li>\n",
    "      <li><b>emb_dim</b> (int): dimensions of the embeddings to read in, valid values: 50, 100, 200, 300</li>\n",
    "      <li><b>enc</b> (str): encoding used to read the embeddings file, default = \"utf8\"</li>\n",
    "    </ul>\n",
    "  </li>\n",
    "  <li>Returns a dictionary with keys that are words in the embeddings vocabulary and values that are emb_dim-dimensional embedding vectors for those words</li>\n",
    "</ul>\n",
    "\n",
    "You can check your version with the one in the `word_embeddings.py` module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing word vectors.\n",
      "Found 400000 word vectors.\n",
      "['the', ',', '.', 'of', 'to'] <class 'numpy.ndarray'> (100,)\n"
     ]
    }
   ],
   "source": [
    "import word_embeddings as we\n",
    "import os\n",
    "\n",
    "BASE_DIR = ''\n",
    "GLOVE_DIR = os.path.join(BASE_DIR, 'embeddings/glove.6B')\n",
    "TEXT_DATA_DIR = os.path.join(BASE_DIR, '20_newsgroup')\n",
    "# MAX_SEQUENCE_LENGTH = 1000\n",
    "# MAX_NUM_WORDS = 20000\n",
    "EMBEDDING_DIM = 100  # there are 4 sizes: 50, 100, 200 and 300\n",
    "\n",
    "# read in the GloVe embeddings - be patient... This is a big file!\n",
    "embeddings = we.get_glove(glove_dir = GLOVE_DIR, emb_dim = EMBEDDING_DIM)\n",
    "# check out a few of the items that we have embeddings for, verify embedding type and shape\n",
    "embedding_keys = list(embeddings.keys())\n",
    "print(embedding_keys[0:5], type(embeddings['the']), embeddings['the'].shape)  # keys are words that have canned embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the vocabulary and pre-process text\n",
    "\n",
    "We'll use the labeled sentiment data from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences). The file **sentiment+labelled+sentences.zip** contains the following four files:\n",
    "\n",
    "+ amazon_cells_labelled.txt\n",
    "+ imdb_labelled.txt\n",
    "+ readme.txt\n",
    "+ yelp_labelled.txt\n",
    "\n",
    "### NOTE: This is an overly simplistic example of text processing. For a more realistic example, see my capstone project notebook:\n",
    "\n",
    "https://github.com/MichaelSzczepaniak/llmamd/blob/main/preproc_disaster.ipynb\n",
    "\n",
    "Download the zip file to a local directory, unzip the file, concatenate the 3 review files (amazon, imdb and yelp) into a single file (I did this manually and called the resulting file: `full_set.txt`) and then do the following:\n",
    "\n",
    "1. Read in the data to be classified: sentiment (1 = positive or 0 = negative)\n",
    "2. Separate reviews from their corresponding labels\n",
    "3. Prepare the text for modeling (pre-processing)\n",
    "    1. Remove punctuation and numbers.\n",
    "    2. Transform all words to lower-case.\n",
    "    3. Remove stop words.\n",
    "    4. Vectorize each word in each review\n",
    "    5. Convert the set of review word vectors into a document vector for each review using the bag-of-words representation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read in the data set.\n",
    "with open(\"sentiment_labelled_sentences/full_set.txt\") as f:\n",
    "    content = f.readlines()\n",
    "    \n",
    "## Remove leading and trailing white space\n",
    "content = [x.strip() for x in content]\n",
    "\n",
    "## Separate the sentences from the labels - tab separates sentences and labels\n",
    "sentences = [x.split(\"\\t\")[0] for x in content]\n",
    "labels = [x.split(\"\\t\")[1] for x in content]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "## replace_with_space takes a string x and a list of characters removal_list \n",
    "## returns x with all the characters in removal_list replaced by ' '\n",
    "def replace_with_space(x, removal_list):\n",
    "    for w in removal_list:\n",
    "        x = x.replace(w, ' ')\n",
    "    return x\n",
    "\n",
    "## Remove digits\n",
    "digits = [str(x) for x in range(10)]\n",
    "digit_less = [replace_with_space(x, digits) for x in sentences]\n",
    "\n",
    "## Remove punctuation\n",
    "punc_less = [replace_with_space(x, list(string.punctuation)) for x in digit_less]\n",
    "\n",
    "## Make everything lower-case\n",
    "sents_lower = [x.lower() for x in punc_less]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "so there is no way for me to plug it in here in the us unless i go by a converter  | label = 0\n",
      "good case  excellent value  | label = 1\n",
      "great for the jawbone  | label = 1\n"
     ]
    }
   ],
   "source": [
    "# check what we have so far\n",
    "for i in [0, 1, 2]:\n",
    "    print(f\"{sents_lower[i]} | label = {labels[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Simple Stop Words Removal\n",
    "\n",
    "Stop words are words that are filtered out because they are believed to contain no useful information for the task at hand. These usually include articles such as 'a' and 'the', pronouns such as 'i' and 'they', and prepositions such as 'to' and 'from'. We have put together a very small list of stop words to keep things simple, but there are much larger lists which can easily be found on the web.\n",
    "\n",
    "### NLTK Stop Words\n",
    "\n",
    "See reference 9. below\n",
    "\n",
    "### spaCy Stop Words\n",
    "\n",
    "See reference 10. below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define our stop words\n",
    "stop_set = set(['the', 'a', 'an', 'i', 'he', 'she', 'they', 'to', 'of', 'it', 'from'])\n",
    "\n",
    "## Split each line into word tokens\n",
    "sents_split = [x.split() for x in sents_lower]\n",
    "# remove stop words then rebuild the sentences\n",
    "sents_processed = [\" \".join(list(filter(lambda a: a not in stop_set, x))) for x in sents_split]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "so there is no way for me plug in here in us unless go by converter | label = 0\n",
      "good case excellent value | label = 1\n",
      "great for jawbone | label = 1\n"
     ]
    }
   ],
   "source": [
    "# check what we have so far\n",
    "for i in [0, 1, 2]:\n",
    "    print(f\"{sents_processed[i]} | label = {labels[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build vocaculary, convert words to vectors, convert word vectors to document vectors using the bag-of-words representation\n",
    "\n",
    "We will build the vocabulary by capping the number of words (features, tokens) to the top 4500 most frequently occuring words. This means that a word will make it into our vocabulary only if it is one of the 4500 most common words in the corpus. This is often a useful step as it can weed out spelling mistakes and words which occur too infrequently to be useful.\n",
    "\n",
    "We will come back to the one hot encoded reviews when we're ready to build the logistic regression model. At this point, we just building the vocabulary that will be used to explore word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "selected_vocab_size = 4500\n",
    "\n",
    "## Transform to BOW representation - start by instantiating a vectorizer object\n",
    "vectorizer = CountVectorizer(analyzer = \"word\", tokenizer = None,\n",
    "                             preprocessor = None, stop_words = None, max_features = selected_vocab_size)\n",
    "data_features = vectorizer.fit_transform(sents_processed)\n",
    "\n",
    "## convert to 3d vector (matrix): each row is a review, each col corresponds a word count for each sample\n",
    "data_mat = data_features.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['so', 'there', 'is', 'no', 'way', 'for', 'me', 'plug', 'in', 'here'],\n",
       " (3000, 4500),\n",
       " array([[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]], dtype=int64))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voc_dict = vectorizer.vocabulary_  # keys are words in the vocabulary, values are the total counts of each word in all the reviews\n",
    "vocab = list(voc_dict.keys())\n",
    "vocab[0:10], data_mat.shape, data_mat[0:3, 0:3]  # 3000 reviews = docs, 4500 words = vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example - Logistic Regression to classify reviews\n",
    "\n",
    "We'll build a model that classifies product reviews as either positive or negative.  We'll start by stating the mathematical formulation for logistic regression.  This model learns the conditional probability that a given data point $x$ is in one of two classes: $y = $ 1 or -1:\n",
    "\n",
    "<img src=\"./graphics/0.3 logistic regression summary.png\">\n",
    "\n",
    "### Figure 2.\n",
    "\n",
    "The nice thing about this formulation is that it can be used unaltered to predict either class if we designate the two classes by $y \\in \\{-1, 1\\}$\n",
    "<br>\n",
    "<br>\n",
    "<font style=\"font-size:18px\">DQ1: Do you remember the typical approach to solving for these parameters?</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the data\n",
    "\n",
    "In real-life, we'd probably spend a bunch of time wrangling data collected from one or more sources like websites or databases.  After that, you'll need to spend even more time annotating these reviews (1.).\n",
    "<br>\n",
    "<br>\n",
    "<font style=\"font-size:18px\">DQ2: What is _annotating_ and why do we need to do this?</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll side-step much of this work by looking at [labeled sentiment data from the UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences). The file `sentiment_labelled_sentences.zip` contains the following four files:\n",
    "\n",
    "+ amazon_cells_labelled.txt\n",
    "+ imdb_labelled.txt\n",
    "+ readme.txt\n",
    "+ yelp_labelled.txt\n",
    "\n",
    "The file `full_set.txt` used below was created by manually appending the `amazon`, `imdb`, and `yelp` files together.\n",
    "\n",
    "This is what the first few rows look like:\n",
    "\n",
    "|text |sentiment|\n",
    "|:----|---------|\n",
    "|So there is no way for me to plug it in here in the US unless I go by a converter.|0|\n",
    "|Good case, Excellent value|1|\n",
    "|Great for the jawbone.|1|\n",
    "|Tied to charger for conversations lasting more than 45 minutes.MAJOR PROBLEMS!!|0|\n",
    "|The mic is great.|1|\n",
    "\n",
    "The data set consists of 3000 sentences, each labeled '1' (if it came from a positive review) or '0' (if it came from a negative review). To be consistent with our formulation above, we will change the negative review label to '-1'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import string  # to get access to punctuation chars\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.rc('xtick', labelsize=14) \n",
    "matplotlib.rc('ytick', labelsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read in the data set.\n",
    "with open(\"sentiment_labelled_sentences/full_set.txt\") as f:\n",
    "    content = f.readlines()\n",
    "    \n",
    "## Remove leading and trailing white space\n",
    "content = [x.strip() for x in content]\n",
    "\n",
    "## Separate the sentences from the labels - tab separates sentences and labels\n",
    "sentences = [x.split(\"\\t\")[0] for x in content]\n",
    "labels = [x.split(\"\\t\")[1] for x in content]\n",
    "\n",
    "## Transform the labels from '0 vs. 1' to '-1 vs. 1'\n",
    "y = np.array(labels, dtype='int8')\n",
    "y = 2*y - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000 (3000,)\n",
      "Great for the jawbone. | label: 1\n",
      "Tied to charger for conversations lasting more than 45 minutes.MAJOR PROBLEMS!! | label: -1\n"
     ]
    }
   ],
   "source": [
    "print(len(content), y.shape)\n",
    "print(f\"{sentences[2]} | label: {labels[2]}\") # positive +1 example\n",
    "print(f\"{sentences[3]} | label: {y[3]}\") # positive -1 example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From Text to Vectors\n",
    "\n",
    "<img src=\"./graphics/0.4 text to vector.png\" height=\"386\" width=\"246\" >\n",
    "\n",
    "### Figure 3.\n",
    "\n",
    "The central idea around word embeddings is that they are an alternative way to represent text as vectors.  But how might we do this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of words and One-Hot Encoding\n",
    "\n",
    "The classic approach to doing this conversion is to use a _bag of words_ (**BOW**) representation.\n",
    "\n",
    "<img src=\"./graphics/0.5 bag of words.png\" height=\"600\" width=\"400\" >\n",
    "\n",
    "### Figure 4.\n",
    "\n",
    "In this representation, each word is first mapped to a binary number with all zeros and a single 1.  For example, say we had a 7 word vocabulary $V$ consisting of the following text:\n",
    "\n",
    "_Wes likes to walk in the park_\n",
    "\n",
    "If each word vector were stacked horizontally we would form the following matrix:  <br><br>\n",
    "\n",
    "\n",
    "\\begin{bmatrix}\n",
    "1 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 1 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 1 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 1 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 1 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 1 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0 & 1 \n",
    "\\end{bmatrix}\n",
    "where\n",
    "\n",
    "Wes = [1, 0, 0, 0, 0, 0, 0]  \n",
    "likes = [0, 1, 0, 0, 0, 0, 0]  \n",
    "to = [0, 0, 1, 0, 0, 0, 0]  \n",
    "walk = [0, 0, 0, 1, 0, 0, 0]  \n",
    "in = [0, 0, 0, 0, 1, 0, 0]  \n",
    "the = [0, 0, 0, 0, 0, 1, 0]  \n",
    "park = [0, 0, 0, 0, 0, 0, 1]  \n",
    "\n",
    "In **BOW**, Sentences, documents (collections of sentences) and collections of documents are reprsented by a V-dimensional vector $x$, where $x_i$ is the number of times that word $i$ occurs in the sentence.  So if we only had this sentence in a single document, it would be represented as:\n",
    "\n",
    "\\begin{bmatrix}\n",
    "1 \\\\\n",
    "1 \\\\\n",
    "1 \\\\\n",
    "1 \\\\\n",
    "1 \\\\\n",
    "1 \\\\\n",
    "1 \n",
    "\\end{bmatrix}\n",
    "\n",
    "If we added these two sentences to our document:\n",
    "\n",
    "_Wes likes to walk_\n",
    "\n",
    "_Wes likes to park_\n",
    "\n",
    "the vector representing this 3 sentence document would look like this:\n",
    "\n",
    "\\begin{bmatrix}\n",
    "3 & = \\text{'Wes' count} \\\\\n",
    "3 & = \\text{'likes' count} \\\\\n",
    "3 & = \\text{'to' count} \\\\\n",
    "2 & = \\text{'walk' count} \\\\\n",
    "1 & = \\text{'in' count} \\\\\n",
    "1 & = \\text{'the' count} \\\\\n",
    "2 & = \\text{'park' count} \n",
    "\\end{bmatrix}\n",
    "\n",
    "\n",
    "More realistically, a native speaker typically knows 15 - 20k words, but even though we probably don't need this many words to do a decent job classifying text, this vector still gets rather large.\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<font style=\"font-size:18px\">DQ3: Assuming the size of these vectors are not a problem, what might be some drawbacks to this type of representation?</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to BOW Inputs\n",
    "\n",
    "### NOTE: This is an overly simplistic example of text processing. For a more realistic example, see my capstone project notebook:\n",
    "\n",
    "https://github.com/MichaelSzczepaniak/llmamd/blob/main/preproc_disaster.ipynb\n",
    "\n",
    "### Simple text processing steps\n",
    "\n",
    "Before we can convert the data to numeric vectors, we have a little wrangling to do. This will consist of the following four transformations:\n",
    "\n",
    "1. Remove punctuation and numbers.\n",
    "2. Transform all words to lower-case.\n",
    "3. Remove _stop words_.\n",
    "4. Convert the sentences into vectors using the bag-of-words representation.\n",
    "\n",
    "The next cell handles the first two steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## replace_with_space takes a string x and a list of characters removal_list \n",
    "## returns x with all the characters in removal_list replaced by ' '\n",
    "def replace_with_space(x, removal_list):\n",
    "    for w in removal_list:\n",
    "        x = x.replace(w, ' ')\n",
    "    return x\n",
    "\n",
    "## Remove digits\n",
    "digits = [str(x) for x in range(10)]\n",
    "digit_less = [replace_with_space(x, digits) for x in sentences]\n",
    "\n",
    "## Remove punctuation\n",
    "punc_less = [replace_with_space(x, list(string.punctuation)) for x in digit_less]\n",
    "\n",
    "## Make everything lower-case\n",
    "sents_lower = [x.lower() for x in punc_less]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['so there is no way for me to plug it in here in the us unless i go by a converter ',\n",
       " 'good case  excellent value ',\n",
       " 'great for the jawbone ']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents_lower[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['so there is no way for me plug in here in us unless go by converter',\n",
       " 'good case excellent value',\n",
       " 'great for jawbone']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents_processed[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['so', 'there', 'is', 'no', 'way', 'for', 'me', 'plug', 'in', 'here'],\n",
       " (3000, 4500),\n",
       " array([[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]], dtype=int64))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voc_dict = vectorizer.vocabulary_\n",
    "vocab = list(voc_dict.keys())\n",
    "vocab[0:10], data_mat.shape, data_mat[0:3, 0:3]  # 3000 reviews = docs, 4500 words = vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3516, 3900, 1925)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at a few word counts\n",
    "voc_dict['so'], voc_dict['there'], voc_dict['is']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the shape of `data_mat`, it looks like the data is layed out with each **row being a sample** and each **column a feature**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training / Testing Split\n",
    "\n",
    "Here we split the data into a training set of 2500 sentences and a test set of 500 sentences (of which 250 are positive and 250 negative)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## split the data into testing and training sets\n",
    "np.random.seed(0)\n",
    "# indices of 250 - and 250 + random samples, need the [0] on the np.where calls because\n",
    "# it returns a 2-tuple and the results of the 1st logical condition are all we want\n",
    "test_inds = np.append(np.random.choice((np.where(y==-1))[0], 250, replace=False),\n",
    "                      np.random.choice((np.where(y==1))[0], 250, replace=False))\n",
    "# build training set from indices not in the test set\n",
    "train_inds = list(set(range(len(labels))) - set(test_inds))\n",
    "\n",
    "train_data = data_mat[train_inds,]\n",
    "train_labels = y[train_inds]\n",
    "\n",
    "test_data = data_mat[test_inds,]\n",
    "test_labels = y[test_inds]\n",
    "\n",
    "print(\"train data: \", train_data.shape)\n",
    "print(\"test data: \", test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting a logistic regression model to the training data\n",
    "\n",
    "Because solving for the parameters of the logistic regression model above is a **convex optimization**, we could implement our own logistic regression solver using stochastic gradient descent (SGD), but we'll use the solver already in `scikit-learn`.  Due to the randomness of the SGD procedure, different runs can yield slightly different solutions and thus different error values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "## fit logistic classifier on training data: minimize neg log likelihood (\"log\"), no regularization penalty\n",
    "clf = SGDClassifier(loss=\"log\", penalty=\"none\")\n",
    "clf.fit(train_data, train_labels)\n",
    "\n",
    "## Pull out the parameters (w,b) of the logistic regression model\n",
    "w = clf.coef_[0,:]\n",
    "b = clf.intercept_\n",
    "\n",
    "## Get predictions on training and test data\n",
    "preds_train = clf.predict(train_data)\n",
    "preds_test = clf.predict(test_data)\n",
    "\n",
    "## Compute errors\n",
    "errs_train = np.sum((preds_train > 0.0) != (train_labels > 0.0))\n",
    "errs_test = np.sum((preds_test > 0.0) != (test_labels > 0.0))\n",
    "\n",
    "print(\"Training error: \", float(errs_train)/len(train_labels))\n",
    "print(\"Test error: \", float(errs_test)/len(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data, test_data.shape, test_labels.shape  # reminder of what our test shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute probability of each test point\n",
    "preds_test_probs = clf.predict_proba(test_data)[:,1]\n",
    "preds_test_probs[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_test[0:10]  # low probabilities map to -1, higher prob's to +1 which is a good check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create ROC Curves\n",
    "\n",
    "ROC curves are an excellent tool to evaluate and compare binary classifiers.  The following function allows us to create a set of ROC curves based on the **labels** and the **probabilities** assigned to each row in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "\n",
    "# set default plot size - https://stackoverflow.com/questions/36367986\n",
    "plt.rcParams['figure.figsize'] = [10, 10]\n",
    "\n",
    "def get_roc_curves(y_tests, y_scores, model_names=None, plot_title='Comparing Model ROCs', colors=None):\n",
    "    \"\"\"\n",
    "    Creates a plot of ROC curves and their corresponding AUC values for a set of models.\n",
    "\n",
    "    Args:\n",
    "          y_tests(np.array(int)): 2-d array where each column are the binary class labels\n",
    "          (0 or 1) for a particular model. This array MUST have either a single column OR\n",
    "          the same number of columns as y_scores.\n",
    "          \n",
    "          If y_tests is a single column vector, then function assumes that the same y_tests\n",
    "          values should be a applied to each column in y_scores.\n",
    "          \n",
    "          y_scores(np.array(float)): 2-d array where rows are samples and each column are\n",
    "          the probabilities that each corresponding y_tests value = 1 for a particular model\n",
    "          \n",
    "          model_names(list(str)): list of size y_tests.shape[1] = y_scores.shape[1]\n",
    "          which are the names of the models used to generate each score column. If no model\n",
    "          names are passed in (default), generic names of the form \"model x\" will be created\n",
    "          where x is an integer in [0, y_tests.shape[1])\n",
    "\n",
    "          colors(list(str)): a list of colors. If None (default) function will use the 10 color\n",
    "          Tableau pallette: grey or grey, brown, orange, olive, green, cyan, blue, purple, pink, red\n",
    "\n",
    "    Returns:\n",
    "        2-tuple: First item is a matplotlib.pyplot object which has a show() method which renders the plot.\n",
    "        Second item is a dict with keys that are the model_names and values that are the AUC\n",
    "        of the True Positive Rate vs False Positive Rate (ROC) curve for that model.\n",
    "                 \n",
    "    \"\"\"\n",
    "    \n",
    "    # ensure single dim vectors are 1D column vectors so they can be sliced consistenly later on\n",
    "    if len(y_tests.shape) == 1:\n",
    "        y_tests = y_tests.reshape(-1, 1)\n",
    "    if len(y_scores.shape) == 1:\n",
    "        y_scores = y_scores.reshape(-1, 1)\n",
    "        \n",
    "    n_models = y_scores.shape[1]\n",
    "    \n",
    "    # check shapes of the true labels (y_test) and model-computed probabilities (y_scores)\n",
    "    if y_tests.shape[1] > 1 and y_scores.shape[1] != y_tests.shape[1]:\n",
    "        print(\"get_roc_curves ERROR: \")\n",
    "        print(\"y_tests has {} columns, y_scores has {} columns\".format(y_tests.shape[1],\n",
    "                                                                       y_scores))\n",
    "        return False\n",
    "    elif y_tests.shape[1] == 1 and y_scores.shape[1] > 1:\n",
    "        print(\"DEBUG get_roc_curves: BEFORE expanding y_tests from 1 to {} columns\".format(y_scores.shape[1]))\n",
    "        # If y_tests is a single column vector and n_models > 1, add copies of the single y_tests column\n",
    "        y_tests = np.reshape(y_tests, (-1, 1))\n",
    "        print(\"DEBUG get_roc_curves: BEFORE expansion, y_tests shape = {}\".format(y_tests.shape))\n",
    "        y_expanded = np.copy(y_tests)\n",
    "        print(\"DEBUG get_roc_curves: BEFORE expansion, y_expanded shape = {}\".format(y_expanded.shape))\n",
    "        for i in range(n_models-1):\n",
    "            y_expanded = np.hstack((y_expanded, y_tests))\n",
    "            print(\"DEBUG get_roc_curves: DURING expansion, i = {} \".format(i))\n",
    "            print(\"DEBUG get_roc_curves: DURING expansion, y_expanded shape = {} \".format(y_expanded.shape))\n",
    "        y_tests = y_expanded\n",
    "        print(\"DEBUG get_roc_curves: AFTER expansion, y_tests columns = {} \".format(y_tests.shape[1]))\n",
    "    \n",
    "    print(f\"Comparing {n_models} models\")\n",
    "    # If no model names are passed in, create generic names\n",
    "    if model_names == None:\n",
    "        model_names = ['model' + str(i) for i in range(n_models)]\n",
    "    \n",
    "    plt.figure()\n",
    "    lw = 2\n",
    "    if colors == None:\n",
    "        # https://stackoverflow.com/questions/22408237/named-colors-in-matplotlib\n",
    "        colors = ['tab:grey', 'tab:blue', 'tab:orange', 'tab:red', 'tab:purple',\n",
    "                  'tab:green', 'tab:cyan', 'tab:brown', 'tab:olive', 'tab:pink']\n",
    "    \n",
    "    color_count = len(colors)\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    # compute true pos rate and false pos rate over range of thresholds and AUC for each model\n",
    "    for i in range(n_models):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_tests[:, i], y_scores[:, i])\n",
    "        roc_auc[model_names[i]] = auc(fpr[i], tpr[i])\n",
    "    \n",
    "    # plot reference line: random classifier\n",
    "    plt.plot([0, 1], [0, 1], color=colors[0], lw=lw, linestyle='--')\n",
    "    # add traces for each model\n",
    "    for j in range(0, n_models):\n",
    "        plt.plot(fpr[j], tpr[j], color=colors[j % color_count + 1],\n",
    "                 lw=lw, label=model_names[j] + ' (AUC = %0.2f)' % roc_auc[model_names[j]])\n",
    "    \n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate', fontsize=14)\n",
    "    plt.ylabel('True Positive Rate', fontsize=14)\n",
    "    plt.title(plot_title, fontsize=14)\n",
    "    plt.legend(loc=\"lower right\", fontsize=14)\n",
    "    \n",
    "    return plt, roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels.shape, test_labels.ndim, preds_test_probs.shape, test_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([1,2,3,4])\n",
    "x.shape, x.shape[0], len(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a single ROC curve for the One-Hot encoded logistic regression sentiment model\n",
    "roc_1hot, auc_1hot = get_roc_curves(test_labels, preds_test_probs,\n",
    "                                    model_names=['Logistic Regression - BOW'],\n",
    "                                    plot_title='Product Review Sentiment ROCs')\n",
    "roc_1hot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My personal preference is to start with a simple and **interpretable** model to serve as a baseline for comparison before getting into the more sophisticated models. The model we just built will serve this purpose.\n",
    "\n",
    "## Interpretation of BOW Coefficients\n",
    "\n",
    "We used a **BOW** representation in order to vectorize each sentiment. For such a simple representation, this does a pretty decent job, but how do we interpret this model?\n",
    "\n",
    "<font style=\"font-size:18px\">DQ4: Think back to our earlier question about the pros and cons of the _one-hot encoded (1HE) inputs_.\n",
    "<br>\n",
    "<br>\n",
    "a) In general, how many features will we have for models built like this?\n",
    "<br>\n",
    "<br>\n",
    "b) What does each coefficient represent?</font>\n",
    "<br>\n",
    "<br>\n",
    "Now that we have a BOW-based version, we'll work on building the same model but using word embeddings.  But before we do that, lets review some of the relevant concepts around representation and unsupervised learning techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative Representation:  Word Embeddings (WE's) \n",
    "\n",
    "How does a word embedding vector compare to a one-hot encoded word vector?  We know what the later looks like: a single 1 in one location and 0 everywhere else.  Let's take a look at what a word embedding looks like.\n",
    "\n",
    "For the code below to work, you need to download the GloVe (canned) embeddings from the link provide in the **Reference** section below (ref 5.) into the same directory as your notebook or else set the **DIR** parameters below accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "BASE_DIR = ''\n",
    "GLOVE_DIR = os.path.join(BASE_DIR, 'glove.6B')\n",
    "TEXT_DATA_DIR = os.path.join(BASE_DIR, '20_newsgroup')\n",
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "MAX_NUM_WORDS = 20000\n",
    "EMBEDDING_DIM = 100  # there are 4 sizes: 50, 100, 200 and 300\n",
    "\n",
    "\n",
    "def get_embeddings(glove_dir = GLOVE_DIR, emb_dim = EMBEDDING_DIM, enc = \"utf8\"):\n",
    "    \"\"\"\n",
    "    Reads in and returns a specified set of embeddings.\n",
    "    \n",
    "    Args:\n",
    "        glove_dir(str): directory containing the GloVe embeddings\n",
    "        \n",
    "        emb_dim(int): dimensions of the embeddings to read in, valid values: 50, 100, 200, 300\n",
    "        \n",
    "        enc(str): encoding used to read the embeddings file, default = \"utf8\"\n",
    "    \n",
    "    \n",
    "    Returns(dict): a dictionary with keys that are words in the embeddings vocabulary\n",
    "    and values that are emb_dim-dimensional embedding vectors for those words\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    print('Indexing word vectors.')\n",
    "    # load the embeddings into a dict with keys that are words and\n",
    "    # values are the embedding vectors for those words\n",
    "    embedding_index = {}\n",
    "\n",
    "    with open(os.path.join(glove_dir, 'glove.6B.' + str(emb_dim) + 'd.txt'), encoding=enc) as f:\n",
    "        for line in f:\n",
    "            word, coeffs = line.split(maxsplit = 1)\n",
    "            coeffs = np.fromstring(coeffs, dtype='float', sep=' ')\n",
    "            embedding_index[word] = coeffs\n",
    "        \n",
    "    print(\"Found {} word vectors.\".format(len(embedding_index)))\n",
    "    \n",
    "    return embedding_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = get_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check out a few of the items that we have embeddings for, verify embedding type and shape\n",
    "embedding_keys = list(embeddings.keys())\n",
    "embedding_keys[0:5], type(embeddings['the']), embeddings['the'].shape  # keys are words that have canned embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabulary Embeddings\n",
    "\n",
    "As we saw above, our GloVe download contains embeddings for 400,000 tokens (words and punctuation).  Since we have defined our vocabulary to be much smaller than this, we'll use the function below to create a dict of embeddings for just the items in our vocabulary so we aren't slinging around a bigger set of embeddings than we need to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab_embeddings(embedding_index, voc = vocab):\n",
    "    \"\"\"\n",
    "    Gets the embeddings for the words in a vocabulary\n",
    "    \n",
    "    Args:\n",
    "        embedding_index(dict): a dictionary with keys that are words in the\n",
    "        embeddings vocabulary and values that are embedding vectors for those words\n",
    "        \n",
    "        voc(list(str)): list of words that we want embeddings for\n",
    "    \n",
    "    \n",
    "    Returns(tuple): 2-tuple where the first element is a dict of vocabulary word\n",
    "    keys and embedding vector values. The second element is a list of the words\n",
    "    that were not found in the embedding_index dict sorted in alphabetical order\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # get embeddings for just the words in our vocabulary\n",
    "    voc_embeddings = {}\n",
    "    no_embeddings = []\n",
    "    for word in voc:\n",
    "        emb = embedding_index.get(word, None)\n",
    "        if type(emb) == np.ndarray:\n",
    "            voc_embeddings[word] = embedding_index.get(word, None)\n",
    "        else:\n",
    "            no_embeddings.append(word)\n",
    "#         print(\"No embedding for {}\".format(word))\n",
    "    \n",
    "    return voc_embeddings, sorted(no_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_embeddings, missing_words = get_vocab_embeddings(embeddings, vocab)\n",
    "words_found = len(vocab_embeddings)\n",
    "words_missing = len(missing_words)\n",
    "print(\"got embeddings for {} out of {} words in our vocabulary. Missing {}\".format(words_found,\n",
    "                                                                                   len(vocab),\n",
    "                                                                                   words_missing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# which words are missing?\n",
    "missing_words[0:5]  # names and funny mispellings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What does the embedding for 'the' look like?\n",
    "vocab_embeddings['the'], len(vocab_embeddings['the'])  # 100 real numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 100 Dimensional Vector of Reals vs. 4500 Dimensional One-Hot Vector\n",
    "\n",
    "###  What kind of information might we want the vector of reals to contain?\n",
    "\n",
    "How about similar words having vectors that are similar?  The function below computes the nearest neighboring word in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_NN(w, vocab_embeddings):\n",
    "    \"\"\"\n",
    "    Finds the word closest to w in the vocabulary that isn't w itself.\n",
    "    \n",
    "    Args:\n",
    "        w(str): string, word to compute nearest neighbor for - must be in a key in vocab_embeddings\n",
    "        vocab_embeddings(dict): dictionary with keys that are words in the vocabulary\n",
    "          and values that are d-dimensional numpy array of floats that are the real-\n",
    "          vector embeddings for each word in the vocabulary\n",
    "          \n",
    "    Returns:\n",
    "        string: the word in the vocabulary that is the closest to this particular word\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    vocab_words = set(vocab_embeddings.keys())\n",
    "    # check if the word passed in is in the vocabulary\n",
    "    if not(w in vocab_words):\n",
    "        print (\"Unknown word\")\n",
    "        return\n",
    "    \n",
    "    # remove the word we are looking for the nearest neighbor of\n",
    "    vocab_words.discard(w)\n",
    "    vocab_words = list(vocab_words)\n",
    "    \n",
    "    # get the embedding for passed in word\n",
    "    w_embedding = vocab_embeddings[w]\n",
    "    neighbor = 0\n",
    "    curr_dist = np.linalg.norm(w_embedding - vocab_embeddings[vocab_words[0]])\n",
    "    # iterate through all the words in the vocabulary and find the 'closest'\n",
    "    for i in vocab_words:\n",
    "        dist = np.linalg.norm(w_embedding - vocab_embeddings[i])\n",
    "        if (dist < curr_dist):\n",
    "            neighbor = i\n",
    "            curr_dist = dist\n",
    "            \n",
    "    return neighbor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do similar word embeddings equate to similar word semantics?\n",
    "\n",
    "Let's try a few."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab_words.index('money')  # 60\n",
    "word_NN('money', vocab_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_NN('bought', vocab_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_NN('loved', vocab_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_NN('above', vocab_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_list = sorted(list(vocab_embeddings.keys()))\n",
    "vocab_list[0:10], vocab_list.index('above')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Word Embeddings For Classification\n",
    "\n",
    "Word Embeddings are typically used with neural networds when doing classification.  They are not normally used in logistic regression models because we can't account for the sequence of words easily and it's tricky to properly represent documents without losing a lot of information.\n",
    "\n",
    "Nonetheless, we'll give this a try and see how it works and then compare our results the bag-of-words (BOW) model we built earlier.  We'll need to first figure out how to represent documents using word embeddings.  References 6 and 7 below suggest that using a concatenated vector of coordinate-wise minimums and maximimums works well, so we'll give this a try with 300-dimensional embeddings instead of the 100-dimensional embeddings we were using earlier for illustration purposes.  The means that each document will be represented by a vector in $\\mathbb{R}^{2d}$ or $\\mathbb{R}^{600}$.  We'll draw some pictures so we can get a clear understanding of what's going on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representing Document as Vectors\n",
    "\n",
    "If we wrote each d-dimensional word embedding in a document of $n$ words as a column vector and stacked them horizontally, we would form the following $d \\times n$ matrix:\n",
    "\n",
    "<img src=\"./graphics/doc2vec01a.png\">\n",
    "\n",
    "#### Figure 6.  Document represented as a matrix of embedding vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graphic above represents a document as a $d \\times n$ matrix, but what we need is a representation of a document as a **vector**.  We'll do this by collapsing all $n$ columns in this matrix into a single column by first taking **coordinate minimums** which are the minimums of each row.  We'll then do the same, but take **coordinate maximums** and then stacking them on top of each other.  When do this, each document $i$ becomes of fixed length vector that looks like this:\n",
    "\n",
    "<img src=\"./graphics/doc2vec02c.png\">\n",
    "\n",
    "#### Figure 7.  Feature vector representing a single document derived from matrix described in Figure 6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of these $x_i$ vectors are samples, so we'll need to transpose this vector and stack them horizontally to form our data (feature) matrix which we'll send to our solver.\n",
    "\n",
    "We'll start the process by reading the 300-dimensional embeddings, pare these back to our vocabulary and then build our feature matrix.  This is also referred to as a _design matrix_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_embs300 = get_embeddings(glove_dir = GLOVE_DIR, emb_dim = 300, enc = \"utf8\")\n",
    "print(\"size of embeddings vocabulary: {}\".format(len(vocab_embs300)))\n",
    "vocab_embs300, missing_embs = get_vocab_embeddings(vocab_embs300, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"We have {} words in our vocabulary\".format(selected_vocab_size))\n",
    "print(\"embeddings found for our vocabulary: {}\".format(len(vocab_embs300)))\n",
    "words_not_in_embeddings = selected_vocab_size - len(vocab_embs300)\n",
    "print(\"vocabulary words not found in embeddings: {}\".format(words_not_in_embeddings))\n",
    "the_emb = vocab_embs300['the']\n",
    "print(\"shape of embeddings read in: {}\".format(the_emb.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the documents to matrices of embeddings\n",
    "\n",
    "The function below converts a document to the matrix described in the Figure 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc2matrix(doc, voc_embs, emb_dims=300):\n",
    "    \"\"\"Converts a string (doc) representing a document into a (emb_dims x n)\n",
    "    matrix of word embeddings where n is the number of words in doc.\n",
    "    \n",
    "    Args:\n",
    "        doc(str): list(str) where each string is a review (document aka doc) consisting of\n",
    "        space delimited words\n",
    "        \n",
    "        voc_embs(dict): dictionary with keys that are the words in the selected vocabulary and\n",
    "        values that are the emb_dims-dimensional word embeddings for those words.\n",
    "        \n",
    "        emb_dims(int): size of the word embeddings used to encode words in voc_embs\n",
    "    \n",
    "    Returns:\n",
    "        numpy.ndarray: emb_dims x n array of floats representing a doc\n",
    "        \n",
    "        n is the number of word tokens in doc\n",
    "    \n",
    "    \"\"\"\n",
    "    doc_words = doc.split()\n",
    "    raw_doc = np.zeros((emb_dims, 1))\n",
    "    # build the d x n matrix of embeddings as a raw representation of doc\n",
    "    for i, word in enumerate(doc_words):\n",
    "#         print(word)\n",
    "        if i == 0:\n",
    "            if word in voc_embs:\n",
    "                raw_doc = voc_embs[word].reshape(-1, 1)\n",
    "            else:\n",
    "                raw_doc = np.zeros((emb_dims, 1))\n",
    "                \n",
    "#             print(\"initializeing raw_doc: {}\".format(raw_doc.shape))\n",
    "            continue\n",
    "        else:\n",
    "            # use zero vector for words that have no embedding\n",
    "            if word in voc_embs:\n",
    "                word_emb = voc_embs[word].reshape(-1, 1)\n",
    "#                 print(word_emb, word_emb.shape)\n",
    "            else:\n",
    "                word_emb = np.zeros((emb_dims, 1))\n",
    "            \n",
    "#         print(raw_doc.shape, \" | \", word_emb.shape)\n",
    "        raw_doc = np.hstack((raw_doc, word_emb))\n",
    "        \n",
    "        \n",
    "    return raw_doc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate through each review, convert to Figure 6. matrices and store in a list\n",
    "docs_as_matrices = []\n",
    "for sent in sents_processed:\n",
    "    sent_matrix = doc2matrix(sent, vocab_embs300, 300)\n",
    "    docs_as_matrices.append(sent_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"review strings/docs processed: {}\".format(len(docs_as_matrices)))\n",
    "print(\"first review text: {}\".format(sents_processed[0]))\n",
    "print(\"shape of the first review: {}\".format(docs_as_matrices[0].shape))\n",
    "print(\"top left corner of first review matrix: \\n{}\".format(docs_as_matrices[0][0:3, 0:3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the document matrices to single vectors\n",
    "\n",
    "The function below converts a document matrix described in the Figure 6. into a single coordinate min/max $\\mathbb{R}^{600}$ vector described in Figure 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_x_min_max(doc_as_embs):\n",
    "    \"\"\"Creates the coordinate min/max vector representation for a document\n",
    "    represented as horizontally stacked word embeddings.\n",
    "    \n",
    "    Args:\n",
    "        doc_as_embs (numpy.ndarray): document represented as a d x n matrix\n",
    "            where d is the size of the word embeddings and n is the number\n",
    "            of words in the document being represented\n",
    "    \n",
    "    Returns:\n",
    "        numpy.ndarray: real vector column vector of length 2d where d i\n",
    "        is the size of the word embeddings\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    min_vec = np.amin(doc_as_embs, axis=1).reshape(-1, 1)\n",
    "    max_vec = np.amax(doc_as_embs, axis=1).reshape(-1, 1)\n",
    "    x_vec = np.vstack((min_vec, max_vec))\n",
    "    \n",
    "    return x_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_as_matrices[0].shape, docs_as_matrices[1].shape  # should be (embedding dim x number of words in doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_emb_feature_matrix(docs_as_matrices):\n",
    "    \"\"\"Converts a list of documents matrices as described in Figure 6.\n",
    "    into a matrix of document vectors with columns described by\n",
    "    Figure 7.\n",
    "    \n",
    "    Args:\n",
    "        docs_as_matrices (list): list where each element is a (d x w)\n",
    "            matrix of floats where d is the length of the embeddings\n",
    "            vectors representing the words in each document and w is\n",
    "            the number of word tokens in each document\n",
    "    \n",
    "    Returns:\n",
    "        numpy.ndarray: matrix of floats of shape (2d x n) where n is\n",
    "        the number of input documents that are to be used as inputs\n",
    "        to the model\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    # iterate through the list of documents represented as embedding vectors and create the min and max vectors\n",
    "    for i, doc_as_matrix in enumerate(docs_as_matrices):\n",
    "        x_vec = get_x_min_max(doc_as_matrix)\n",
    "#      print(i, x_vec.shape)\n",
    "        if i == 0:\n",
    "            x_features = np.copy(x_vec.T)\n",
    "#             print(\"intialize x_feature: {}\".format(x_features.shape))\n",
    "        else:\n",
    "            x_features = np.vstack((x_features, x_vec.T))\n",
    "#             print(\"vstack x_feature: {}\".format(x_features.shape))\n",
    "    \n",
    "    return x_features\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_feats = create_emb_feature_matrix(docs_as_matrices)\n",
    "x_feats.shape  # should be (doc count x (2 x embedding dim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model With New Features\n",
    "\n",
    "We'll use the same data to train and test another model from WE inputs. Below we'll repeat the same train/test split as we did for the first model, but using the embeddings form of the inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_embs = x_feats[train_inds,]\n",
    "train_labels_embs = y[train_inds]\n",
    "\n",
    "test_data_embs = x_feats[test_inds,]\n",
    "test_labels_embs = y[test_inds]\n",
    "\n",
    "print(\"train data: \", train_data_embs.shape)\n",
    "print(\"test data: \", test_data_embs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## fit logistic classifier on training data: minimize neg log likelihood (\"log\"), no regularization penalty\n",
    "clf_embs = SGDClassifier(loss=\"log\", penalty=\"none\")\n",
    "clf_embs.fit(train_data_embs, train_labels_embs)\n",
    "\n",
    "## Pull out the parameters (w,b) of the logistic regression model\n",
    "w_embs = clf_embs.coef_[0,:]\n",
    "b_embs = clf_embs.intercept_\n",
    "\n",
    "## Get predictions on training and test data\n",
    "preds_train_embs = clf_embs.predict(train_data_embs)\n",
    "preds_test_embs = clf_embs.predict(test_data_embs)\n",
    "\n",
    "## Compute errors\n",
    "errs_train_embs = np.sum((preds_train_embs > 0.0) != (train_labels_embs > 0.0))\n",
    "errs_test_embs = np.sum((preds_test_embs > 0.0) != (test_labels_embs > 0.0))\n",
    "\n",
    "print(\"Training error: \", float(errs_train_embs)/len(train_labels_embs))\n",
    "print(\"Test error: \", float(errs_test_embs)/len(test_labels_embs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_test_probs_glove = clf_embs.predict_proba(test_data_embs)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_test_probs_bow_v_glove = np.hstack((preds_test_probs.reshape(-1,1),\n",
    "                                          preds_test_probs_glove.reshape(-1,1)))\n",
    "preds_test_probs.shape, preds_test_probs_glove.shape, preds_test_probs_bow_v_glove.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_test_probs_bow_v_glove\n",
    "\n",
    "# create a two ROC curves comparing BOW and WE encoded logistic regression sentiment models\n",
    "roc_bow_v_glove, auc_1hot = get_roc_curves(test_labels, preds_test_probs_bow_v_glove,\n",
    "                                           model_names=['LR - BOW, |V|=4500',\n",
    "                                                        'LR - min/max 600-dim GloVe'],\n",
    "                                           plot_title='Product Review Sentiment ROCs')\n",
    "roc_bow_v_glove.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Figure 8.  Logistic Regression using BOW $\\mathbb{R}^{4500}$ (blue) vs. GloVe min/max $\\mathbb{R}^{600}$ (orange)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font style=\"font-size:18px\">DQ5: What might Figure 8. be telling us?  Specifically:\n",
    "<br>\n",
    "<br>\n",
    "a) Which of these two representations contains more information *per dimension*?  Why?\n",
    "<br>\n",
    "<br>\n",
    "b) How do you think these representations might compare if they were inputs to a CNN instead of a logistic regression model?</font>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Word Embeddings as they were intended\n",
    "\n",
    "The fact that we didn't see any improvement using word embeddings in the way we chose (document min/max) with logistic regression shouldn't have been too surprising because of the way logistic regression works: learning a linear decision boundary\n",
    "\n",
    "We could probably do better if we continued to experiment with different ways to represent document with the embeddings, but why might we be optimistic about this if logistic regression just learns a linear decision boundary in high-dimensional space?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Word Embeddings\n",
    "\n",
    "There are generally two approaches to implementing word embeddings in an analysis.  One can:\n",
    "\n",
    "1. use pre-trained (aka \"canned\") embeddings or\n",
    "2. create customer embeddings\n",
    "\n",
    "### Pros and Cons of Pre-Trained Embeddings\n",
    "\n",
    "\n",
    "### Pros and Cons of Custom Embeddings\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Trained Embeddings\n",
    "\n",
    "The two alogritms I'm aware of for creating pre-trained word embeddings are:\n",
    "\n",
    "1. Word2Vec\n",
    "2. GloVe - Global Vectors for Word Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec\n",
    "\n",
    "Two different learning models were introduced that can be used as part of the word2vec approach to learn the word embedding; they are:\n",
    "\n",
    "+ Continuous Bag-of-Words, or CBOW model.\n",
    "+ Continuous Skip-Gram Model.\n",
    "\n",
    "The CBOW model learns the embedding by predicting the current word based on its context. The continuous skip-gram model learns by predicting the surrounding words given a current word.\n",
    "\n",
    "The continuous skip-gram model learns by predicting the surrounding words given a current word.\n",
    "\n",
    "<img src=\"./graphics/Word2Vec-Training-Models.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Figure 9. Word2Vec Training Models Taken from “Efficient Estimation of Word Representations in Vector Space”, 2013 (ref. 15.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both models are focused on learning about words given their local usage context, where the context is defined by a window of neighboring words. This window is a configurable parameter of the model.\n",
    "\n",
    "#### Meta-parameters\n",
    "\n",
    "To build embeddings using the **Word2Vec** algorithm, the following meta-parameters need to be set:\n",
    "\n",
    "+ CBOW vs. Skip-Gram - described above\n",
    "+ Sliding Window Size - In the above diagam, window size = 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GloVe\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A simple approach to word embeddings\n",
    "\n",
    "*You shall know a word by the company it keeps. (J.R. Firth, 1957)*\n",
    "\n",
    "+ Much of the meaning of a word w is captured by the words it co-occurs with:\n",
    "<p><center>$w_{t-3} \\quad w_{t-2} \\quad w_{t-1} \\quad w_t \\quad w_{t+1} \\quad w_{t+2} \\quad w_{t+3}$</center>\n",
    "    \n",
    "<img src=\"./graphics/simple_we_approach.png\">\n",
    "\n",
    "1. **Get a list of words and their frequencies** - We'll recompute these using the **Brown** corpus.   \n",
    "2. **Decide on the vocabulary** - There are two potentially distinct vocabularies: the words for which we will obtain embeddings (`vocab_words`) and the words we will consider when looking at context information (`context_words`). We will take the former to be all words that occur at least 20 times, and the latter to be all words that occur at least 100 times. These choices are pretty arbitrary: by all means, play around with them and find something better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Accessing the Brown corpus\n",
    "\n",
    "The *Brown corpus* is available as part of the Python Natural Language Toolkit (`nltk`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import nltk\n",
    "nltk.download('brown')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import brown, stopwords\n",
    "from scipy.cluster.vq import kmeans2\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The corpus consists of 500 samples of text drawn from a wide range of sources. When these are concatenated, they form a very long stream of over a million words, which is available as `brown.words()`. Let's look at the first 20 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    print (brown.words()[i],)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we did before, let's remove stopwords and punctuation and make everything lowercase.  In this case, we'll use the NLTK stop words instead of our small manual list like we did originally.\n",
    "\n",
    "The resulting sequence will be stored in `my_word_stream`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_stopwords = set(stopwords.words('english'))\n",
    "# make lower case and remove stopwords in one line\n",
    "word_stream = [str(w).lower() for w in brown.words() if w.lower() not in my_stopwords]\n",
    "# remove all single char words and punctuation\n",
    "my_word_stream = [w for w in word_stream if (len(w) > 1 and w.isalnum())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing co-occurrence probabilities\n",
    "\n",
    "<font color=\"magenta\">Get a list of words and their frequencies.</font>\n",
    "\n",
    "### WARNING:  This is an expensive operation which took over a minute and half to run the first time!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(my_word_stream)\n",
    "words = []\n",
    "totals = {}\n",
    "for i in range(1, N-1):\n",
    "    w = my_word_stream[i]\n",
    "    if w not in words:\n",
    "        words.append(w)\n",
    "        totals[w] = 0\n",
    "    totals[w] = totals[w] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two Vocabularies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_frequency_threshold = 19\n",
    "context_frequency_threshold = 99\n",
    "\n",
    "vocab_words = [w for w in words if totals[w] > embedding_frequency_threshold]\n",
    "context_words = [w for w in words if totals[w] > context_frequency_threshold]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How large are these two word lists? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vocab_words), len(context_words)  # should be (4720, 918), check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Get co-occurrence counts. These are defined as follows, for a small constant `window_size=2`.\n",
    "\n",
    "* Let `w0` be any word in `vocab_words` and `w` any word in `context_words`.\n",
    "* Each time vocabulary word `w0` occurs in the corpus, look at the window of `window_size` words before and after it. If `w` appears in this window, we say it appears in the context of (this particular occurrence of) `w0`.\n",
    "* Define `counts[w0][w]` as the total number of times `w` occurs in the context of `w0`.\n",
    "\n",
    "The function `get_counts` computes the `counts` array, and returns it as a dictionary (with values that are also dictionaries)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_counts(my_word_stream=my_word_stream, window_size=2):\n",
    "    \"\"\"Creates a co-occurence counts of context word counts for each vocabulary word\n",
    "    \n",
    "    PRECONDITION: assumes that following lists are already in the environment:\n",
    "    vocab_words - see above\n",
    "    context_words - see above\n",
    "    \n",
    "    Args:\n",
    "        my_word_stream (list): list of strings that is the overall vocabulary,\n",
    "            stopwords and single char's removed, all lower cased\n",
    "        window_size (int): the number of words to the left and right of a given \n",
    "            word to be considered 'in the context of' that word\n",
    "    \n",
    "    Returns:\n",
    "        dict: Keys are vocabulary words. Value are dictionary with keys that are\n",
    "        context words and values that are counts of those words in the context of\n",
    "        the vocabulary word\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    N = len(my_word_stream)\n",
    "    counts = {}  # holds number of times context word is found to occur (value)\n",
    "                 # with a vocabulary word (key)\n",
    "    # init vocaulary words as keys - values will be another dict\n",
    "    # inner dict with have keys that are context words and\n",
    "    #                      values that are counts that the context word occurs with vocabulary word\n",
    "    for w0 in vocab_words:\n",
    "        counts[w0] = {}\n",
    "    # \n",
    "    for i in range(window_size, N - window_size):\n",
    "        w0 = my_word_stream[i]\n",
    "        # if the stream word is not in the vocabulary, skip it\n",
    "        if w0 in vocab_words:\n",
    "                      # indices to define context window j-2, j-1, j+1, j+2\n",
    "            for j in (list(range(-window_size,0)) + list(range(1,window_size+1))):\n",
    "                w = my_word_stream[i+j]\n",
    "                if w in context_words:\n",
    "                    if w not in counts[w0].keys():\n",
    "                        counts[w0][w] = 1\n",
    "                    else:\n",
    "                        counts[w0][w] = counts[w0][w] + 1\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define `probs[w0][]` to be the distribution over the context of `w0`, that is:\n",
    "* `probs[w0][w] = counts[w0][w] / (sum of all counts[w0][])`\n",
    "  + count of co-occurances divided by sum of all co-occurances\n",
    "\n",
    "This is computed by the function `get_co_occurrence_dictionary`, given `counts`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_co_occurrence_dictionary(counts):\n",
    "    \"\"\"Create the the co-occurance probabilities\n",
    "    \n",
    "    Args:\n",
    "        counts (dict): Keys are vocabulary words. Value are dictionary with keys\n",
    "            that are context words and values that are counts of those words in \n",
    "            the context of the vocabulary word\n",
    "        \n",
    "    Returns:\n",
    "        dict: dictionary with keys that are vocabulary words and values are\n",
    "              dictionaries with keys that are context words and values are\n",
    "              co-occurance probabilities\n",
    "    \n",
    "    \"\"\"\n",
    "    probs = {}\n",
    "    for w0 in counts.keys():\n",
    "        sum_all_cooccurs = 0\n",
    "        for w in counts[w0].keys():\n",
    "            sum_all_cooccurs = sum_all_cooccurs + counts[w0][w]\n",
    "        if sum_all_cooccurs > 0:\n",
    "            # init inner dict of context word prob's for this vocabulary word\n",
    "            probs[w0] = {}\n",
    "            # iterate through all the context words found for this vocabulary word\n",
    "            for w in counts[w0].keys():\n",
    "                probs[w0][w] = float(counts[w0][w]) / float(sum_all_cooccurs)\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final piece of information we need is the frequency of different context words. The function below, `get_context_word_distribution`, takes `counts` as input and returns (again, in dictionary form) the array:\n",
    "\n",
    "* `context_frequency[w]` = sum of all `counts[][w]` / sum of all `counts[][]` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context_word_distribution(counts):\n",
    "    \"\"\"Computes the frequency of different context words\n",
    "    \n",
    "    Args:\n",
    "        counts (dict): Keys are vocabulary words. Value are dictionary with keys\n",
    "            that are context words and values that are counts of those words in \n",
    "            the context of the vocabulary word\n",
    "    \n",
    "    Returns:\n",
    "        dict: \n",
    "    \n",
    "    \"\"\"\n",
    "    counts_context = {}\n",
    "    sum_context = 0\n",
    "    context_frequency = {}\n",
    "    for w in context_words:\n",
    "        counts_context[w] = 0\n",
    "    for w0 in counts.keys():\n",
    "        for w in counts[w0].keys():\n",
    "            counts_context[w] = counts_context[w] + counts[w0][w]\n",
    "            sum_context = sum_context + counts[w0][w]\n",
    "    for w in context_words:\n",
    "        context_frequency[w] = float(counts_context[w])/float(sum_context)\n",
    "    return context_frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the various pieces of information above, we compute the **pointwise mutual information matrix (PMI)**:\n",
    "* `PMI[i,j] = MAX(0, log probs[ith vocab word][jth context word] - log context_frequency[jth context word])`\n",
    "\n",
    "<img src=\"./graphics/pmi.png\">\n",
    "\n",
    "The embedding of any word can then be taken as the corresponding row of this matrix. However, to reduce the dimension, we will apply PCA.\n",
    "\n",
    "### WARNING:  This is also an expensive operation!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Computing counts and distributions\")\n",
    "counts = get_counts(my_word_stream=my_word_stream, window_size=2)\n",
    "probs = get_co_occurrence_dictionary(counts)\n",
    "context_frequency = get_context_word_distribution(counts)\n",
    "#\n",
    "print (\"Computing pointwise mutual information\")\n",
    "n_vocab = len(vocab_words)\n",
    "n_context = len(context_words)\n",
    "pmi = np.zeros((n_vocab, n_context))\n",
    "for i in range(0, n_vocab):\n",
    "    w0 = vocab_words[i]\n",
    "    if w0 in probs.keys():\n",
    "        for w in probs[w0].keys():\n",
    "            j = context_words.index(w)\n",
    "            pmi[i,j] = max(0.0, np.log(probs[w0][w]) - np.log(context_frequency[w]))\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now reduce the dimension of the PMI vectors using principal component analysis. Here we bring it down to 100 dimensions, and then normalize the vectors to unit length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=100)\n",
    "vecs = pca.fit_transform(pmi)\n",
    "for i in range(0,n_vocab):\n",
    "    vecs[i] = vecs[i]/np.linalg.norm(vecs[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# save this embedding so that it doesn't need to be computed every time.\n",
    "fd = open(\"embedding.pickle\", \"wb\")\n",
    "pickle.dump(vocab_words, fd)\n",
    "pickle.dump(context_words, fd)\n",
    "pickle.dump(vecs, fd)\n",
    "fd.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vecs[0]), n_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_NN_by_index(w, vocab_words, vecs):\n",
    "    if not(w in vocab_words):\n",
    "        print (\"Unknown word\")\n",
    "        return\n",
    "    v = vecs[vocab_words.index(w)]\n",
    "    neighbor = 0\n",
    "    curr_dist = np.linalg.norm(v - vecs[0])\n",
    "    for i in range(1, n_vocab):\n",
    "        dist = np.linalg.norm(v - vecs[i])\n",
    "        if (dist < curr_dist) and (dist > 0.0):\n",
    "            neighbor = i\n",
    "            curr_dist = dist\n",
    "    return vocab_words[neighbor]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_NN_by_index('money', vocab_words, vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_NN_by_index('bought', vocab_words, vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_NN_by_index('loved', vocab_words, vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_NN_by_index('above', vocab_words, vecs)  # might be an NLTK stop word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Brown-based embeddings on other words\n",
    "\n",
    "These test cases looked reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_NN_by_index('pulmonary', vocab_words, vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_NN_by_index('world', vocab_words, vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO - These didn't give the same results as earlier testing - Need to debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_NN_by_index('london', vocab_words, vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_NN_by_index('communism', vocab_words, vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two Main Goals of RL: Vector Quantization & Finding Meaningful Structure in Data\n",
    "\n",
    "Word embeddings focus primarily on the former (vector quantization) with the later (meaningful structure) in mind.\n",
    "\n",
    "### Common Clustering Methods\n",
    "\n",
    "+ k-Mean (covered awhile ago,so we'll do a quick review)\n",
    "+ PCA (also covered awhile ago, so we'll do a quick review this also)\n",
    "\n",
    "\n",
    "\n",
    "+ includes (but is not limited to) **dimensionality reduction** \n",
    "  + bring out true degree of freedom\n",
    "+ involves encoding and decoding steps\n",
    "+ results range from **sparse** to **dense encodings**\n",
    "+ typical examples: PCA, k-Means, NN-auto-encoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised Learning\n",
    "\n",
    "## k-Means Review\n",
    "\n",
    "+ Simpliest unsupervised learning algorithm\n",
    "+ Results differ depending on initial choice of centers (means)\n",
    "+ Multiple methods for center intialization\n",
    "  + randomly chose all centers from uniform distribution\n",
    "  + randomly chose extra centers, the prune later\n",
    "  + k-means++\n",
    "    + chose first center from uniform distribution\n",
    "    + let $C = {x}$ centers chosen so far\n",
    "    + pick successive centers from distribution $Pr(x) \\propto dist{(x, C)}^2$\n",
    "      + probability of chosing next center increases as you move away from existing centers\n",
    "+ Results in a **sparse** encoding (more on this later) - just the labels\n",
    "+ The good:\n",
    "  + Fast and easy\n",
    "  + Effective in quantization\n",
    "+ The bad:\n",
    "  + Spherical clusters of the same radius\n",
    "\n",
    "Basic algorithm:\n",
    "\n",
    "1. Choose k\n",
    "2. Initialize k-centers: $\\mu_1,..., \\mu_k$  \n",
    "  + There are several ways to do this. Simpliest is chosing them randomly.\n",
    "3. Assign each point to its closest center\n",
    "4. Update each $\\mu_j$ to the mean of the points assigned to it.\n",
    "5. Repeat 3. and 4. until the $\\mu_j$ values don't move anymore\n",
    "\n",
    "\n",
    "<font color=\"blue\" style=\"font-size:18px\">Graphically, the first 2 iterations of the algorithm looks like this on a small dataset with k = 3</font>:\n",
    "<img src=\"./graphics/04 kmeans_4steps.png\" width=\"600\" height=\"400\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=\"red\" style=\"font-size:18px\">Why reduce the number of features in a data set?</font>**\n",
    "\n",
    "1. reduces storage and computation time\n",
    "2. high-dimensional data often has a lot redundancy\n",
    "3. removing noisy and/or irrelevant features improves model accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**<font color=\"green\" style=\"font-size:16px\">Example: are all the pixels in an image informative?</font>**\n",
    "\n",
    "<img src=\"./graphics/01 informative pixels.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA Review - Eliminating low variance coordinates\n",
    "\n",
    "It took me a couple of revists before getting truly comfortable with **Principal Component Analysis** (PCA), but it was worth the effort because it's one of the most important tools to have in your Data Science toolbox.\n",
    "\n",
    "PCA is based on chosing the highest variance directions in which to project your data onto.  This is accomplished by computing the eigenvalue and accompanying eigenvectors\n",
    "\n",
    "projection directions based on the **covariance matrix** of the data. This matrix allows us to contrast the effect of picking **coordinate** directions (i.e. pixels) versus **eigenvector** directions. In particular:\n",
    "\n",
    "* *The ith **diagonal entry** of the covariance is the variance in the ith coordinate (the ith pixel).*\n",
    "* *The ith **eigenvalue** of the covariance matrix is the variance in the direction of the ith eigenvector.*\n",
    "\n",
    "### Why project data onto another direction?  A one dimensional example\n",
    "\n",
    "<img src=\"./graphics/02 a 1d example.jpg\">\n",
    "\n",
    "If two variables are correlated, doesn't knowing one inform you of the other?  If so, most of the information is contained along the **projection** in the direction of maximimum variation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do we find this \"best\" direction to project onto?\n",
    "\n",
    "<img src=\"./graphics/03 first eigenvector of covar.jpg\">\n",
    "\n",
    "<font style=\"font-size:18px\">By </font> <font color=\"green\" style=\"font-size:18px\"> \"best\" </font><font style=\"font-size:18px\">we mean the direction of maximum variance</font>\n",
    "\n",
    "<font style=\"font-size:18px\">DQx. Why would we want to compute eigenvectors from covariance matrix?</font>\n",
    "\n",
    "<font style=\"font-size:18px\">DQy. What do we mean by the \"first eigenvector of the covariance matrix\"?</font>\n",
    "\n",
    "<font style=\"font-size:18px\">DQz. What would the second eigenvector of the covariance matrix look like for the data shown above?</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion Answers\n",
    "\n",
    "**DQ1.** Start by writing the formulation of the likelihood:  $\\Pi^n_{i=1} Pr_{w,b}(y_i | x_i)$  which is what we want to **maximize**.  We typically turn it into a **minimization** problem by taking the natural log and then the negative of that.  We then search for the minimum of the **negative log-likelihood** using gradient descent to find a solution.  Because this problem is **convex**, we are gauranteed to find a solution.\n",
    "\n",
    "**DQ2.**  In a real-world scenario, you would collect these reviews from an available source, but often times these reviews are not labeled \"positive\" or \"negative\".  When this is the case, you have to read through the reviews yours and assign labels.\n",
    "\n",
    "E-Bay get around this by having the user click a radio button that identifies the review.  This is a good idea as it can save a lot time down the road.\n",
    "\n",
    "**DQ3.** 1) No word context or language structure is accounted for. E.g. \"The dog bit Johnny.\" and \"Johnny bit the dog.\" use the same 4 words, but have very different meanings (especially if you're Johnny or the dog).\n",
    "<br><font style=\"color:white\">**DQ2.** </font>2) Sparse representations are generally require a lot of memory and computation to process (see reference 1. below)\n",
    "\n",
    "**DQ4.** a) We'll have a coefficient for each word in our defined vocabulary plus 1 for the bias term or $|V| + 1$.<br><br>b) Each coefficient represents the relative importance of that words contribution to the probability that the document has a positive sentiment.<br><br>More specifically, a negative coefficient implies that the more prevelant this word is in a document, the **lower** the probability that the sentiment will be positive. Similarly, a positive coefficient implies that the more prevelant this word is in a document, the **higher** the probability that the sentiment will be positive.\n",
    "\n",
    "**DQ5.** a) Because the AUC of both of these representations are very similar, this implies each representation contains a similar amount total of information in the context of logistic regression. Since the WE representation is much smaller than the BOW representation (600 vs 4500 dimensions), the WE representation contains more information *per dimension in the context of logistic regression*.\n",
    "<br><br>b) The WE representation would probably do better than the BOW because more information is contained in this representation and a CNN can more flexibly utilize this information during training\n",
    "\n",
    "**DQx.** For covariance matrix $\\Sigma$ of d-dimensional data $X$, the variance of $X$ in the direction of $u$ is given by $u^T \\Sigma u$.  $u^T \\Sigma u$ is maximized by setting $u$ to the first **eigenvector** of $\\Sigma$.  The maximum value of this variance is the corresponding **eigenvalue**.\n",
    "\n",
    "**DQy.** For d-dimensional $X$, the **eigenvalues** of the covariance matrix are the variances of the data projected in the direction of the corresponding **eigenvector**.  So to find first **principal component (PC)**, we compute all $d$ sets of eigenvectors and eigenvalues ($\\Sigma u_i = \\lambda_i u_i$ for i = 1 to d) and then find the eigenvector ($u_i$) corresponding the largest eigenvalue ($\\lambda_i$)\n",
    "\n",
    "**DQz**. This would be the line perpendicular to the first **PC** because eigenvectors computed from the covariance matrix form an **orthonormal basis**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "1. I had to spend many hours doing annotations for a recent project.  It's interesing for the first hour or two as you learn about a new domain, but can get rather mind-numbing after that.\n",
    "2. [A Gentle Introduction to Sparse Matrices for Machine Learning](https://machinelearningmastery.com/sparse-matrices-for-machine-learning/)\n",
    "3. [tf-idf on Wikipedia](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)\n",
    "4. ROC reference - TODO\n",
    "5. [GloVe: Global Vectors for Word Representation](https://nlp.stanford.edu/projects/glove/)\n",
    "  +  Jeffrey Pennington, Richard Socher, Christopher D. Manning\n",
    "  + Pre-trained word vectors where obtained [here](http://nlp.stanford.edu/data/glove.6B.zip)\n",
    "6. [Apply word embeddings to entire document, to get a feature vector](https://stats.stackexchange.com/questions/221715/apply-word-embeddings-to-entire-document-to-get-a-feature-vector/)\n",
    "7. [Representation learning for very short texts using weighted word embedding aggregation](https://arxiv.org/pdf/1607.00570.pdf)\n",
    "8. [Keras Project - Pre-trained word embeddings example](https://github.com/keras-team/keras/blob/master/examples/pretrained_word_embeddings.py)\n",
    "9. [NLTK stop words](https://pythonspot.com/nltk-stop-words/)\n",
    "10. [Stop Words in spaCy](https://realpython.com/natural-language-processing-spacy-python/#stop-words)\n",
    "11. [spaCy english language model - GloVe-based](https://spacy.io/models/en#en_core_web_lg)\n",
    "12. [What Are Word Embeddings for Text?](https://machinelearningmastery.com/what-are-word-embeddings/)\n",
    "13. [Gender Stereotype Reinforcement](https://arxiv.org/pdf/2009.01334v1.pdf)\n",
    "14. [Neural Network Methods in Natural Language Processing - Goldberg](https://www.amazon.com/Language-Processing-Synthesis-Lectures-Technologies/dp/1627052984/ref=as_li_ss_tl?ie=UTF8&qid=1502062931&sr=8-1&keywords=Neural+Network+Methods+in+Natural+Language+Processing&linkCode=sl1&tag=inspiredalgor-20&linkId=d63df073fea3ebe2d405820570b3ff03)\n",
    "15. [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/pdf/1301.3781.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
